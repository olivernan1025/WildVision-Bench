|                  Model                   | Score |    95% CI   | Win Rate | Reward | Much Better | Better |  Tie  | Worse | Much Worse | Avg Tokens |
| :--------------------------------------: | :---: | :---------: | :------: | :----: | :---------: | :----: | :---: | :---: | :--------: | :--------: |
|                  gpt-4o                  | 89.15 | (-2.2, 2.0) |  80.6%   |  56.4  |    255.0    | 148.0  |  14.0 |  72.0 |    11.0    |    142     |
|           gpt-4-vision-preview           | 79.78 | (-2.6, 2.6) |  71.8%   |  39.4  |    182.0    | 177.0  |  22.0 |  91.0 |    28.0    |    138     |
|       Qwen/Qwen2.5-VL-7B-Instruct        | 76.65 | (-3.3, 2.7) |  70.0%   |  32.1  |    105.0    | 245.0  |  39.0 |  88.0 |    23.0    |    185     |
|                   aria                   |  74.1 | (-2.5, 3.0) |  68.0%   |  28.8  |     88.0    | 252.0  |  47.0 |  86.0 |    27.0    |    185     |
|        google/Gemini-Flash-1.5-8B        | 69.99 | (-2.4, 2.4) |  63.4%   |  22.8  |     93.0    | 224.0  |  27.0 | 130.0 |    26.0    |    116     |
|        Qwen/Qwen2-VL-7B-Instruct         |  68.7 | (-2.7, 3.2) |  58.6%   |  20.4  |    106.0    | 187.0  |  37.0 | 145.0 |    25.0    |    157     |
|                Reka-Flash                | 64.65 | (-2.2, 2.9) |  58.8%   |  18.9  |    135.0    | 159.0  |  28.0 | 116.0 |    62.0    |    168     |
|        mistralai/Pixtral-12B-2409        | 62.85 | (-3.7, 3.4) |  58.6%   |  15.2  |     70.0    | 223.0  |  34.0 | 135.0 |    38.0    |    163     |
|          claude-3-opus-20240229          | 62.03 | (-3.7, 3.0) |  53.0%   |  13.5  |    103.0    | 162.0  |  48.0 | 141.0 |    46.0    |    105     |
| meta-llama/Llama-3.2-90B-Vision-Instruct | 56.53 | (-3.6, 3.2) |  54.0%   |  8.5   |     98.0    | 172.0  |  20.0 | 137.0 |    73.0    |    150     |
|         allenai/Molmo-7B-D-0924          | 55.67 | (-3.4, 3.1) |  54.2%   |  8.2   |     80.0    | 191.0  |  31.0 | 127.0 |    71.0    |    158     |
|                yi-vl-plus                | 55.05 | (-3.2, 3.1) |  52.8%   |  7.2   |     98.0    | 166.0  |  29.0 | 124.0 |    83.0    |    140     |
|        liuhaotian/llava-v1.6-34b         | 51.89 | (-3.0, 3.1) |  49.2%   |  2.5   |     90.0    | 156.0  |  26.0 | 145.0 |    83.0    |    153     |
|         claude-3-sonnet-20240229         |  50.0 |  (0.0, 0.0) |   0.2%   |  0.1   |     0.0     |  1.0   | 499.0 |  0.0  |    0.0     |    114     |
|             neulab/Pangea-7B             | 46.48 | (-2.9, 2.8) |  46.0%   |  -3.1  |     55.0    | 175.0  |  32.0 | 160.0 |    78.0    |    167     |
| meta-llama/Llama-3.2-11B-Vision-Instruct | 44.04 | (-3.9, 3.4) |  45.0%   |  -6.2  |     61.0    | 164.0  |  23.0 | 156.0 |    96.0    |    173     |
|         claude-3-haiku-20240307          | 37.83 | (-4.3, 3.4) |  30.6%   | -16.5  |     54.0    |  99.0  |  47.0 | 228.0 |    72.0    |     89     |
|            gemini-pro-vision             | 35.57 | (-2.8, 2.6) |  32.6%   | -21.0  |     80.0    |  83.0  |  27.0 | 167.0 |   143.0    |     68     |
|     liuhaotian/llava-v1.6-vicuna-13b     | 33.87 | (-3.0, 2.6) |  33.8%   | -21.4  |     62.0    | 107.0  |  25.0 | 167.0 |   139.0    |    136     |
|     deepseek-ai/deepseek-vl-7b-chat      | 33.61 | (-3.0, 2.8) |  35.6%   | -21.2  |     59.0    | 119.0  |  17.0 | 161.0 |   144.0    |    116     |
|           THUDM/cogvlm-chat-hf           | 32.01 | (-2.7, 3.3) |  30.6%   | -26.4  |     75.0    |  78.0  |  15.0 | 172.0 |   160.0    |     61     |
|     liuhaotian/llava-v1.6-vicuna-7b      | 26.41 | (-3.1, 2.3) |  27.0%   | -31.4  |     45.0    |  90.0  |  36.0 | 164.0 |   165.0    |    130     |
|            idefics2-8b-chatty            | 23.96 | (-3.1, 2.6) |  26.4%   | -35.8  |     44.0    |  88.0  |  19.0 | 164.0 |   185.0    |    135     |
|            Qwen/Qwen-VL-Chat             | 18.08 | (-2.1, 2.1) |  19.6%   | -47.9  |     42.0    |  56.0  |  15.0 | 155.0 |   232.0    |     69     |
|             llava-1.5-7b-hf              |  15.5 | (-2.2, 2.2) |  18.0%   | -47.8  |     28.0    |  62.0  |  25.0 | 174.0 |   211.0    |    185     |
|        liuhaotian/llava-v1.5-13b         | 14.43 | (-1.8, 2.3) |  16.8%   | -52.5  |     28.0    |  56.0  |  19.0 | 157.0 |   240.0    |     91     |
|            BAAI/Bunny-v1_0-3B            | 12.98 | (-1.8, 2.0) |  16.6%   | -54.4  |     23.0    |  60.0  |  10.0 | 164.0 |   243.0    |     72     |
|            openbmb/MiniCPM-V             | 11.95 | (-2.0, 2.0) |  13.6%   | -57.5  |     25.0    |  43.0  |  16.0 | 164.0 |   252.0    |     86     |
|         bczhou/tiny-llava-v1-hf          |  8.3  | (-1.4, 1.5) |  11.0%   | -66.2  |     16.0    |  39.0  |  15.0 | 127.0 |   303.0    |     72     |
|     unum-cloud/uform-gen2-qwen-500m      |  7.81 | (-1.3, 1.6) |  10.8%   | -68.5  |     16.0    |  38.0  |  11.0 | 115.0 |   320.0    |     92     |
|      google/paligemma2-10b-mix-448       |  7.12 | (-1.2, 1.3) |  10.0%   | -66.4  |     11.0    |  39.0  |  12.0 | 151.0 |   287.0    |     43     |